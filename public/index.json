[{"authors":["admin"],"categories":null,"content":"I work with Prof. Upamanyu Madhow in the Electrical and Computer Engineering department at UCSB. My research interests include machine learning, signal processing, and wireless communication.\n","date":1588538207,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588538207,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.ece.ucsb.edu/~metehancekic/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/~metehancekic/authors/admin/","section":"authors","summary":"I work with Prof. Upamanyu Madhow in the Electrical and Computer Engineering department at UCSB. My research interests include machine learning, signal processing, and wireless communication.","tags":null,"title":"Metehan Cekic","type":"authors"},{"authors":["Metehan Cekic"],"categories":["Deep Learning"],"content":" Adversarial Machine Learning With the advent of more powerful parallel computation units and huge data, we are able to train much more complex and expressive deep neural networks. That is said, deep neural nets (DNN) found its use in a wide variety of fields, ranging from computer vision to game playing agents. They are performing better on some tasks than even human experts in those fields. Despite their incredible success, it is by now well known that they are susceptible to small and carefully designed perturbations which are imperceptible to humans. The fact that DNN\u0026rsquo;s can easily be fooled is a great problem since they are also used in security critical applications such as self-driving cars. Recently, research community has put a great effort to robustify neural networks against these adversarial examples. Despite great attention of research community, there is not a powerful defense mechanism found, and it is shown that defending against adversarial examples are not an easy goal.\nAs another group working on this field, we share our attack codes as a library. This library is a side product of our research, and since we use this in our research as well, we made sure it works correctly and as mentioned in the original papers. To sum up, deepillusion contains easy to use and properly implemented adversarial methods.\nWe are open to suggestions \u0026ldquo;metehancekic@ucsb.edu\u0026rdquo;.\nDeep Illusion Deep Illusion is a toolbox for adversarial attacks in machine learning. Current version is only implemented for Pytorch models. DeepIllusion is a growing and developing python module which aims to help adversarial machine learning community to accelerate their research. Module currently includes complete implementation of well-known attacks (PGD, FGSM, R-FGSM, CW, BIM etc..). All attacks have an apex(amp) version which you can run your attacks fast and accurately. We strongly recommend that amp versions should only be used for adversarial training since it may have gradient masking issues after neural net gets confident about its decisions. All attack methods have an option (Verbose: False) to check if gradient masking is happening.\nAll attack codes are written in functional programming style, therefore, users can easily call the method function and feed the input data and model to get perturbations. All codes are documented, and contains the example use in their description. Users can easily access the documentation by typing \u0026ldquo;??\u0026rdquo; at the and of the method they want to use in Ipython (E.g FGSM?? or PGD??). Output perturbations are already clipped for each image to prevent illegal pixel values. We are open to contributers to expand the attack methods arsenal.\nWe also include the most effective current approach to defend DNNs against adversarial perturbations which is training the network using adversarially perturbed examples. Adversarial training and testing methods are included in torchdefenses submodule.\nCurrent version is tested with different defense methods and the standard models for verification and we observed the reported accuracies.\nCode can be installed via PyPi:\npip install deepillusion  Maintainers: WCSL Lab, Metehan Cekic, Can Bakiskan, Soorya Gopal\nDependencies  numpy 1.16.4\ntqdm 4.31.1\n torchattacks\n pytorch 1.4.0\napex 0.1 (optional)\n tfattacks\n tensorflow\n jaxattacks\n jax\n Installation The most recent stable version can be installed via python package installer \u0026ldquo;pip\u0026rdquo;, or you can clone it from the git page.\npip install deepillusion  or\ngit clone git@github.com:metehancekic/deep-illusion.git  Example Use As mentioned earlier, our adversarial methods are functional instead of modular type. Therefore, all you need to get the perturbations is feeding input data and its labels along with the attack parameters.\nTo standardize the arguments for all attacks, methods accept attack parameters as a dictionary named as attack_params which contains the necessary parameters for each attack. Furthermore, attack methods get the data properties such as the maximum and the minimum pixel value as another dictionary named data_params. These dictinaries make function calls concise and standard for all methods.\nFollowing code snippets show PGD and FGSM usage.\nfrom deepillusion.torchattacks import PGD, FGSM, RFGSM ##### PGD ###### data_params = {\u0026quot;x_min\u0026quot;: 0., \u0026quot;x_max\u0026quot;: 1.} attack_params = { \u0026quot;norm\u0026quot;: \u0026quot;inf\u0026quot;, \u0026quot;eps\u0026quot;: 8./255, \u0026quot;step_size\u0026quot;: 2./255, \u0026quot;num_steps\u0026quot;: 7, \u0026quot;random_start\u0026quot;: False, \u0026quot;num_restarts\u0026quot;: 1} pgd_args = dict(net=model, x=data, y_true=target, data_params=data_params, attack_params=attack_params, verbose=False, progress_bar=False) perturbs = PGD(**pgd_args) data_adversarial = data + perturbs ##### FGSM ##### data_params = {\u0026quot;x_min\u0026quot;: 0., \u0026quot;x_max\u0026quot;: 1.} attack_params = {\u0026quot;norm\u0026quot;: \u0026quot;inf\u0026quot;, \u0026quot;eps\u0026quot;: 8./255} fgsm_args = dict(net=model, x=data, y_true=target, data_params=data_params, attack_params=attack_params) perturbs = FGSM(**fgsm_args) data_adversarial = data + perturbs  Update Deepillusion is a growing and developing library, therefore we strongly recommend to upgrade deepillusion regularly:\npip install deepillusion --upgrade  Current Version 0.1.9\nModule Structure In case investigation of the source codes are needed, this is how our module is structured:\ndeep-illusion │ README.md │ |───deepillusion | | _utils.py Utility functions | | | |───torchattacks | | │ _fgsm.py Fast Gradient Sign Method | | │ _rfgsm.py Random Start + Fast Gradient Sign Method | | │ _pgd.py Projected Gradient Descent | | │ _cw.py Carlini Wagner Linf | | │ _bim.py Basic Iterative Method | | │ _soft_attacks.py Soft attack functions | | │ | | |───amp | | | │ _fgsm.py Mixed Precision (Faster) - Fast Gradient Sign Method | | | │ _rfgsm.py MP - Random Start + Fast Gradient Sign Method | | | │ _cw.py MP - Carlini Wagner Linf | | | │ _pgd.py MP - Projected Gradient Descent | | | | _soft_attacks.py MP - Soft attack functions | | | | | └───analysis | | │ _perturbation_statistics Perturbations statistics functions | | | |───torchdefenses │ | | _adversarial_train_test.py Adversarial Training - Adversarial Testing | | │ | | └───amp | | │ _adversarial_train_test.py MP (Faster) - Adversarial Training - Adversarial Testing | | | |───tfattacks | | | | | | └───jaxattacks | | | └───tests | fgsm_test.py | fgsmt_test.py | pgd_test.py | bim_test.py | rfgsm_test.py | cw_test.py | test_utils.py  Sources    PyPi page for the code\n   Git repo for the code\n  ","date":1588538207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588538207,"objectID":"e8d9236ea1705c1395c98ad7804f4a83","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/deep-illusion/","publishdate":"2020-05-03T13:36:47-07:00","relpermalink":"/~metehancekic/project/deep-illusion/","section":"project","summary":"Created and published an adversarial ML toolbox 'deepillusion'","tags":["Deep Learning"],"title":"Deep Illusion (Adversarial ML toolbox)","type":"project"},{"authors":["Metehan Cekic","Soorya Gopalakrishnan"],"categories":["Deep Learning"],"content":"","date":1583449288,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583449288,"objectID":"edede198007f6e690bb057cd6852ca27","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/adversarial-machine-learning/","publishdate":"2020-03-05T15:01:28-08:00","relpermalink":"/~metehancekic/project/adversarial-machine-learning/","section":"project","summary":"Combating adversarial perturbations by exploiting sparsity of natural data.","tags":["Deep Learning"],"title":"Sparsifying Front Ends","type":"project"},{"authors":["Metehan Cekic","Can Bakiskan"],"categories":[],"content":"The Batak Game Batak is a card game widely played in Turkey. It is similar to Bridge, where after the deck is dealt, players bid -based on their judgment of the value of their hand- in an auction to determine the trump suit. In Batak, each play consists of 13 tricks. A trick is where each player plays 1 card. The play goes as follows: in the first trick, the player that won the auction starts. They play a card from a non-trump suit. Each player has to play a card from the same suit and increase the card’s value. If they can’t increase they can play a lower value card of the same suit. If they don’t have any card of that suit, they can play a card from the trump suit. If they don’t have any card from the trump suit, they can play a card from any other suit they have. In each trick afterwards, the winner of the previous trick starts and other players play according to the same rules outlined above. There are two important rules. Number one, if you have a card of the same suit as the initial suit of the trick, you have to play and you have to increase if you can. Number two, players can’t play the trump suit as the first card of a trick unless trump suit has been \u0026ldquo;unlocked\u0026rdquo; by a player in an earlier trick by him/her not having the suit of that trick, and playing trump suit. In the end, scores are assigned by the number of tricks each player won. If the winner of the auction scored less than their bid, they are assigned negative of the bid as the score. So the advantage of winning the auction is being able to determine the trump suit, the disadvantage is the high negative score in the case of failure to win tricks as many as the bid. In a game, there are usually 10 plays, the goal of the game is to get the highest score.\nAdvantage Actor-Critic Algorithm (A2C) The main property of the A2C algorithm is that it not only optimizes a policy but also trains a value function for the current state to assess how good is that policy for that specific state. In general, policy-based methods and value-based methods have a number of upsides and downsides. A2C algorithm is designed to combine the benefits of both value based and policy based approaches.\nValue-based algorithms require a value function to be calculated for every possible action, although it is more sample efficient. On the other hand, policy-based methods can easily utilized on continuous action spaces due to its direct optimization of policy. However, low sample efficiency causes them to converge slower.\nA2C algorithm estimates both policy and value function via deep neural network. Value function corresponds to the worth of that state to be in and the policy gives the probabilities of each available actions for that state.\n $ \\pi(a|s;\\theta)$: the probability of action $a$ given the state $s$  $V_{\\pi}(s)$ : How good is it to be in state $s$  The value function is mathematically defined as expected reward of a trajectory after the state that agent is at that time in, which is only dependent on $s$.  $V_{\\pi_{\\theta}}(s)=E_{\\tau}(R_{\\tau}|s_0 =s,\\pi_{\\theta})$  Where $\\tau$ represents the trajectory until game ends. Hereafter, for simplicity we drop $\\theta$ from $\\pi$, which means $\\pi$ is parameterized by $\\theta$. Critic of the agent evaluates the policies by computing the advantage function of specific action in a given state, which is defined as follow: $A_{\\pi}(s,a) = Q_{\\pi}(s,a)−V_{\\pi}(s)$ : The advantage of making action a in state s  Where Q function is another value function which depends on both state and action, particularly it corresponds to the value of an action $a$ at a state $s$. Observe that with given definitions: $V_{\\pi}(s) = 􏰀 \\sum_{a\\in A}\\pi(a|s)Q_{\\pi} (s, a)$  The algorithm utilizes the fact that there is a trajectory obtained from a game play (An agent interacted with the environment with a given policy function) by computing Q values from empirical discounted reward:\n$Q_{\\pi}(s,a) = R =\\sum_{i=0}r_{t_i} \\gamma^i $  where $\\gamma$ corresponds to the how important are the future rewards compared to the current reward (1 means equally important, whereas 0.5 means each time step decreases the value of future reward by half). As we have 2 different outputs for our deep neural network, namely policy and value functions, we have two different loss functions. The value loss is defined as mean squared error (MSE) between empirical discounted reward and the value output. This loss motivates neural net to learn to assess policy better after each iteration.\n$Loss_{value} =􏰀\\sum(R−V_{\\pi}(s;\\theta_v))^2$  Similarly, we want to increase the probability of an action which has positive advantage to increase empirical reward, therefore policy loss is defined by: $Loss_{policy} =\\log\\frac{1}{\\pi(a|s;\\theta_p)} A_{\\pi}(s,a;\\theta_v)−\\beta H(\\pi)$  The loss function includes the last term to increase entropy of the action distribution, in other words, we push neural network to explore more. Typical $\\beta$ value is 0.001, it is a good hyperparamemeter to adjust between exploration and exploitation which is a dilemma we always encounter in reinforcement learning algorithms. More inclination towards higher $\\beta$ values causes neural net to explore more, whereas low $\\beta$ values stick with the locally optimum policy. Asynchronous Advantage Actor-Critic Algorithm (A3C) Asynchronous gradient descent boosts the performance of traditional reinforcement algorithms like A2C and Q learning. The main idea of framework is having a master neural network and updating it with the gradients computed from some other worker networks. In standard reinforcement algorithms we usually use memory mechanism to feed earlier experience of the agent when updating the neural network. We call this concept as \u0026ldquo;experience replay\u0026rdquo;, which is helpful to stabilize learning. Running parallel agents independently stabilizes learning without \u0026ldquo;experience replay\u0026rdquo; because of the stochasticity in the game each agent experiences totally different action-reward trajectory. Moreover, as all agents play the game in parallel, training time is reduced roughly linear in the number of parallel actors.\nOur Model We decided that in order to capture the sequential information within the game we should either keep a list of all played cards explicitly in some form of memory, or we should use an RNN that would learn to extract the underlying temporal information implicitly. We decided to go with the second option as the first option requires much bigger input sizes which in turn requires bigger number of neurons in the model. With this decision, we determined the essential states of the game that a human player uses when playing and made these the inputs to our model. They are: trump suit (One-hot vector of length 4), hand (Boolean vector of length 52) and current and past trick’s cards up to our model’s turn, along with who played them.(Boolean matrix of size 7 by 56) Cards are represented by one-hot vector of length 52, players are represented by one-hot vector of length 4, then these vectors are concatenated to form vector of size 56.\nIn following figure, overall architecture of our model is displayed, with the LSTM unrolled in time to give a better sense of time dependency.\nResults We trained 3 different architectures with Nvidia GeForce GTX 1080 Ti. To be able to evaluate trained agent, we get a couple of people (Who knows and plays the game very well) play the game against random-playing bots similar to trained agent.\nResults are given in following figure. As can be seen from the figure, all architectures we experimented performs similarly and much better than random playing games. However, all architectures perform slightly worse than humans.\nConclusion In this project, we wanted to implement and evaluate the performance of A3C algorithm on the Turkish game called \u0026ldquo;Batak\u0026rdquo;. We also wanted to observe the effect of architecture and optimization method for convergence. As can be seen from the results section, the architectures we tried so far perform more or less same.\nWe trained agent in the game without auction, next step for this project would be to improve bots to be able play with auction (learn and bid accordingly). Moreover, we plan to develop agent by making it play against itself (Instead of random bots).\n","date":1583017002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583017002,"objectID":"b0145d23755ce12d801a161f11d3cad1","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/reinforcement-learning-batak/","publishdate":"2020-02-29T14:56:42-08:00","relpermalink":"/~metehancekic/project/reinforcement-learning-batak/","section":"project","summary":"Teaching a network to play Turkish card game “Batak” by using Reinforcement Learning methods. Asynchronous Advantage Actor Critic (A3C) algorithm is implemented for learning. Game environment for agents is written from scratch. LSTM based network model is used to capture temporal information.","tags":["Deep Learning"],"title":"Reinforcement Learning For The Game Called \"Batak\"","type":"project"},{"authors":["Metehan Cekic","Soorya Gopalakrishnan"],"categories":["Deep Learning","Wireless Communication"],"content":"Radio Frequency Machine Learning  Our goal is to learn\u0026nbsp;RF signatures\u0026nbsp;that can distinguish between devices sending\u0026nbsp;exactly\u0026nbsp;the same message. This is possible due to subtle hardware imperfections (labeled\u0026nbsp;\"nonlinearities\" in the figure below) unique to each device.   Since the information in RF data resides in complex baseband, we employ CNNs with complex-valued weights to learn these signatures. This technique\u0026nbsp;does\u0026nbsp;not use\u0026nbsp;signal domain knowledge and can be used for any wireless protocol. We demonstrate its effectiveness for two protocols -\u0026nbsp;WiFi and ADS-B.   We show that this\u0026nbsp;approach is vulnerable to spoofing\u0026nbsp;when using\u0026nbsp;the entire packet:\u0026nbsp;the CNN focuses on\u0026nbsp;fields containing ID info (eg. MAC ID in WiFi) which can be easily spoofed. When using the preamble alone, reasonably high accuracies are obtained, and performance is significantly enhanced by noise augmentation.   We also study robustness to confounding factors\u0026nbsp;in data collected over multiple days and locations, such as the carrier frequency offset (CFO), which drifts over time, and the wireless channel, which depends on the propagation environment. We show that carefully designed data augmentation is critical for learning robust wireless signatures.  \u0026nbsp;\nPublications Metehan Cekic*,\u0026nbsp;Soorya Gopalakrishnan*,\u0026nbsp;Upamanyu Madhow, \"Robust Wireless Fingerprinting: Generalizing Across Space and Time\", arXiv:2002.10791.       Soorya Gopalakrishnan*,  Metehan Cekic* ,\u0026nbsp;Upamanyu Madhow, \"Robust Wireless Fingerprinting via Complex-Valued Neural Networks\", IEEE Global Communications Conference (Globecom),\u0026nbsp;Waikoloa, Hawaii, Dec. 2019.     ","date":1582670974,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582670974,"objectID":"e47c116437f582cd66bd2091001f7d45","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/mywireless-fingerprinting/","publishdate":"2020-02-25T14:49:34-08:00","relpermalink":"/~metehancekic/project/mywireless-fingerprinting/","section":"project","summary":"Extracting circuit-level fingerprints to distinguish between wireless devices sending exactly same message","tags":["Deep Learning","Wireless Communication"],"title":"Radio Frequency Machine Learning (RFML)","type":"project"},{"authors":["Can Bakiskan","Soorya Gopalakrishnan","Metehan Cekic","Upamanyu Madhow","Ramtin Pedarsani"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a485c61333ee5fff2ae5be7419e7702e","permalink":"https://www.ece.ucsb.edu/~metehancekic/publication/bakiskan-2020-polarizing/","publishdate":"2020-02-29T04:44:16.123687Z","relpermalink":"/~metehancekic/publication/bakiskan-2020-polarizing/","section":"publication","summary":"","tags":null,"title":"Polarizing Front Ends for Robust CNNs","type":"publication"},{"authors":["Metehan Cekic","Soorya Gopalakrishnan","Upamanyu Madhow"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f7d0eabf98f7b67d5c322131c35a8e6b","permalink":"https://www.ece.ucsb.edu/~metehancekic/publication/cekic-2020-robust/","publishdate":"2020-02-29T04:44:16.123149Z","relpermalink":"/~metehancekic/publication/cekic-2020-robust/","section":"publication","summary":"","tags":null,"title":"Robust Wireless Fingerprinting: Generalizing Across Space and Time","type":"publication"},{"authors":["Soorya Gopalakrishnan","Metehan Cekic","Upamanyu Madhow"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"fe1ee3c77a8bfe9394ff4940ac777fc5","permalink":"https://www.ece.ucsb.edu/~metehancekic/publication/fingerprinting-2019-globecom/","publishdate":"2020-02-29T04:44:16.122164Z","relpermalink":"/~metehancekic/publication/fingerprinting-2019-globecom/","section":"publication","summary":"","tags":null,"title":"Robust Wireless Fingerprinting via Complex-Valued Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"https://www.ece.ucsb.edu/~metehancekic/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/~metehancekic/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]