[{"authors":["admin"],"categories":null,"content":"I am currently a PhD student, working with Prof. Upamanyu Madhow in the Electrical and Computer Engineering department at UCSB. My research interests include machine learning, signal processing, and wireless communication.\nThrough my research experience, I have developed interests in Deep Learning and its applications. I have worked so far on various areas spanning from Deep Learning in Natural language processing and Computer Vision to Deep Q learning for game playing agents. And Currently I devoted myself to further understand the nature of neural nets and improve the adversarial robustness of neural nets.\n","date":1590442314,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1590442314,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.ece.ucsb.edu/~metehancekic/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/~metehancekic/authors/admin/","section":"authors","summary":"I am currently a PhD student, working with Prof. Upamanyu Madhow in the Electrical and Computer Engineering department at UCSB. My research interests include machine learning, signal processing, and wireless communication.\nThrough my research experience, I have developed interests in Deep Learning and its applications. I have worked so far on various areas spanning from Deep Learning in Natural language processing and Computer Vision to Deep Q learning for game playing agents.","tags":null,"title":"Metehan Cekic","type":"authors"},{"authors":["Metehan Cekic"],"categories":[],"content":"Machine learning and especially deep learning have gained significant attention with the advance in the parallel computation units (GPU, TPU, etc). Higher computational power allowed us to be able to train more complex and expressive neural networks which perform even better than human experts in a number of fields. AlphaGo which is developed by deepmind to play the game of Go was able to beat 18-time world champion Lee Sedol on March 2016. Furthermore, in a well known image classification benchmark imagenet DNNs have already surpassed human performance. All these successes boosted research on DNNs even further.\nDespite deep networks incredible performance in classification tasks, they have recently been shown that they are easy to fool with carefully designed small perturbations to their input which is imperceptible to humans. Recent studies show that most of the well-performing neural networks can succesfully be fooled with a tiny perturbation. With these results, a great effort put forward by the research community to account for this phenomenon in recent years.\nOn this blog, we summarize the most recent adversarial attack and adversarial defense methods. Moreover, we try to give the intuituion behind all these methods so that reader can develop his/her own ideas on top of the state of the art methods. Here we need to note that there is no simple defense mechanism which is robust to all kind of adversarial examples like human visual system. It\u0026rsquo;s been shown that defending against threat models is not an easy task. One can say that even human visual system is not a robust one since it can be easily fooled by illusions. However, our main goal in deep neural networks is that we want to make sure they work similar to human visual system so that they will not be fooled by any kind of perturbations that are imperceptible to humans.\nNeural Networks One needs to understand how deep neural networks are trained to perform a wide variety of tasks to be able to digest how threat models are developed and performed. Bear with me, I will try to give quick summary of neural networks. Deep neural networks consist of serially connected layers which are simply connected by nonlinear activation functions (ReLU, sigmoid, tanh, etc.). These layers are nothing but matrix multiplications which are specifically tuned for the task at hand.\n$$ out=f_L(\u0026hellip;f_1(W_1 * f_0(W_0 * x+b_0))+b_1)\u0026hellip;) \\quad \\quad \\text{: Neural network as a function} $$ Where $W_l$ and $b_l$ corresponds to the parameters of $l$th layer and $f_l$ corresponds to the $l$th layer activation function.\nIn order to train neural network we need labels and a well-defined loss function to approximate these labels. Labels are basically class names for a clasification task (imagine a binary classification with 2 labels: cat, dog) which are expressed as one hot vectors ([1,0] for cat, [0,1] for dog). Output of the neural net will be an array consisting of scalar numbers corresponding to the probabilities of each class (for our example cat and dog e.g [0.9, 0.1] =\u0026gt; probability of cat and dog for given image respectively). Computation of loss is rather easy, we need to define loss function such that if our predictions are close to the original label loss should be small else it hould be high. There are a number of different loss functions one of which (the most popular one) is cross entropy loss defined as:\n$$ Loss = -\\sum_{c=1}^{M} y_c \\log(out_c) \\quad \\quad \\text{: Cross entropy loss for multiclass} $$\nwhere $c$ corresponds for the class index, $y$ corresponds for the one hot label and out corresponds for the prediction propabilities of our neural network. If given label is corresponding to class c then we want to make sure prediction of class c is close to one (1 means neural net thinks that the given image is 100% from that class) so that loss decreased. Observe that negative sign in front of the loss is there to make Loss function positive because we have prediction probabilities in between 0 and 1 which makes $log$ output negative. On the other hand if $y$ corresponding to class c is zero we don\u0026rsquo;t care about the prediction of that class at all.\nNow we need to train our neural network so that we can decrease loss and predict close to the original labels. How can we do that? The answer is simple, and comes from the calculus. All we need to do is that taking derivative of loss with respect to the parameters of the neural network and subtract it from the parameters to decrease loss function. This operation is called gradient descent. First question comes into mind after all these explanition is that how can one compute the derivatives after all serially connected layers, especially for intermediate layers. Don\u0026rsquo;t worry about this though, because back-propagation algorithm is doing the magic for us, and calculate all the derivatives (gradients). We will not discuss the details of back-propagation algorighm since it is beyond the scope of this talk. With the help of back-propagation and gradient descent we train our network with training dataset to decrease loss.\n$$ W = W - \\nabla_{W}(Loss) \\quad \\quad \\text{: Gradient step for all parameters} $$\n$$ w_{il} = w_{il} - \\frac{\\partial Loss }{\\partial w_{il}} \\quad \\quad \\text{: Partial derivative wrt a specific parameter} $$\nWhere $w_{il}$ corresponds to layer $l$s $i$th parameter. To summarize, since gradient gives the direction to increase the given function, by subtracting it, we are able to decrease loss (thanks calculus).\nAfter a number of iterations (epochs), neural network becomes ready to perform classification task. With a distinct test set one can evaluate the trained model to see the performance of it. So far, we learned how the standard training work let us dive into how we generate adversarial examples which fool the neural network we trained.\nGenerating Adversarial Examples We mentioned the power gradient descent, and how it is able to decrease loss function significantly over training process. Can you believe gradient descent is also a tool to create adversarial images for our neural network? Things are getting excited now, \u0026hellip;\n","date":1590442314,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590442314,"objectID":"c422e005d41631db92c0b89dd8e733f8","permalink":"https://www.ece.ucsb.edu/~metehancekic/post/myadversarial-machine-learning/","publishdate":"2020-05-25T14:31:54-07:00","relpermalink":"/~metehancekic/post/myadversarial-machine-learning/","section":"post","summary":"Despite DNNs imperessive performance in wide variety of fields, they are easy to fool with carefully designed small perturbations to their input which is imperceptible to humans.","tags":[],"title":"Adversarial Machine Learning","type":"post"},{"authors":["Metehan Cekic"],"categories":["Deep Learning"],"content":" Adversarial Machine Learning With the advent of more powerful parallel computation units and huge data, we are able to train much more complex and expressive deep neural networks. That is said, deep neural nets (DNN) found its use in a wide variety of fields, ranging from computer vision to game playing agents. They are performing better on some tasks than even human experts in those fields. Despite their incredible success, it is by now well known that they are susceptible to small and carefully designed perturbations which are imperceptible to humans. The fact that DNN\u0026rsquo;s can easily be fooled is a great problem since they are also used in security critical applications such as self-driving cars. Recently, research community has put a great effort to robustify neural networks against these adversarial examples. Despite great attention of research community, there is not a powerful defense mechanism found, and it is shown that defending against adversarial examples are not an easy goal.\nAs another group working on this field, we share our attack codes as a library. This library is a side product of our research, and since we use this in our research as well, we made sure it works correctly and as mentioned in the original papers. To sum up, deepillusion contains easy to use and properly implemented adversarial methods.\nWe are open to suggestions \u0026ldquo;metehancekic@ucsb.edu\u0026rdquo;.\nDeep Illusion Deep Illusion is a toolbox for adversarial attacks in machine learning. Current version is only implemented for Pytorch models. DeepIllusion is a growing and developing python module which aims to help adversarial machine learning community to accelerate their research. Module currently includes complete implementation of well-known attacks (PGD, FGSM, R-FGSM, CW, BIM etc..). All attacks have an apex(amp) version which you can run your attacks fast and accurately. We strongly recommend that amp versions should only be used for adversarial training since it may have gradient masking issues after neural net gets confident about its decisions. All attack methods have an option (Verbose: False) to check if gradient masking is happening.\nAll attack codes are written in functional programming style, therefore, users can easily call the method function and feed the input data and model to get perturbations. All codes are documented, and contains the example use in their description. Users can easily access the documentation by typing \u0026ldquo;??\u0026rdquo; at the and of the method they want to use in Ipython (E.g FGSM?? or PGD??). Output perturbations are already clipped for each image to prevent illegal pixel values. We are open to contributers to expand the attack methods arsenal.\nWe also include the most effective current approach to defend DNNs against adversarial perturbations which is training the network using adversarially perturbed examples. Adversarial training and testing methods are included in torchdefenses submodule.\nCurrent version is tested with different defense methods and the standard models for verification and we observed the reported accuracies.\nMaintainers: WCSL Lab, Metehan Cekic, Can Bakiskan, Soorya Gopal\nDependencies  numpy 1.16.4\ntqdm 4.31.1\n torchattacks\n pytorch 1.4.0\napex 0.1 (optional)\n tfattacks\n tensorflow\n jaxattacks\n jax\n Installation The most recent stable version can be installed via python package installer \u0026ldquo;pip\u0026rdquo;, or you can clone it from the git page.\npip install deepillusion  or\ngit clone git@github.com:metehancekic/deep-illusion.git  Example Use As mentioned earlier, our adversarial methods are functional instead of modular type. Therefore, all you need to get the perturbations is feeding input data and its labels along with the attack parameters.\nTo standardize the arguments for all attacks, methods accept attack parameters as a dictionary named as attack_params which contains the necessary parameters for each attack. Furthermore, attack methods get the data properties such as the maximum and the minimum pixel value as another dictionary named data_params. These dictinaries make function calls concise and standard for all methods.\nFollowing code snippets show PGD and FGSM usage.\nfrom deepillusion.torchattacks import PGD, FGSM, RFGSM ##### PGD ###### data_params = {\u0026quot;x_min\u0026quot;: 0., \u0026quot;x_max\u0026quot;: 1.} attack_params = { \u0026quot;norm\u0026quot;: \u0026quot;inf\u0026quot;, \u0026quot;eps\u0026quot;: 8./255, \u0026quot;step_size\u0026quot;: 2./255, \u0026quot;num_steps\u0026quot;: 7, \u0026quot;random_start\u0026quot;: False, \u0026quot;num_restarts\u0026quot;: 1} pgd_args = dict(net=model, x=data, y_true=target, data_params=data_params, attack_params=attack_params, verbose=False, progress_bar=False) perturbs = PGD(**pgd_args) data_adversarial = data + perturbs ##### FGSM ##### data_params = {\u0026quot;x_min\u0026quot;: 0., \u0026quot;x_max\u0026quot;: 1.} attack_params = {\u0026quot;norm\u0026quot;: \u0026quot;inf\u0026quot;, \u0026quot;eps\u0026quot;: 8./255} fgsm_args = dict(net=model, x=data, y_true=target, data_params=data_params, attack_params=attack_params) perturbs = FGSM(**fgsm_args) data_adversarial = data + perturbs  Update Deepillusion is a growing and developing library, therefore we strongly recommend to upgrade deepillusion regularly:\npip install deepillusion --upgrade  Current Version 0.1.9\nModule Structure In case investigation of the source codes are needed, this is how our module is structured:\ndeep-illusion │ README.md │ |───deepillusion | | _utils.py Utility functions | | | |───torchattacks | | │ _fgsm.py Fast Gradient Sign Method | | │ _rfgsm.py Random Start + Fast Gradient Sign Method | | │ _pgd.py Projected Gradient Descent | | │ _cw.py Carlini Wagner Linf | | │ _bim.py Basic Iterative Method | | │ _soft_attacks.py Soft attack functions | | │ | | |───amp | | | │ _fgsm.py Mixed Precision (Faster) - Fast Gradient Sign Method | | | │ _rfgsm.py MP - Random Start + Fast Gradient Sign Method | | | │ _cw.py MP - Carlini Wagner Linf | | | │ _pgd.py MP - Projected Gradient Descent | | | | _soft_attacks.py MP - Soft attack functions | | | | | └───analysis | | │ _perturbation_statistics Perturbations statistics functions | | | |───torchdefenses │ | | _adversarial_train_test.py Adversarial Training - Adversarial Testing | | │ | | └───amp | | │ _adversarial_train_test.py MP (Faster) - Adversarial Training - Adversarial Testing | | | |───tfattacks | | | | | | └───jaxattacks | | | └───tests | fgsm_test.py | fgsmt_test.py | pgd_test.py | bim_test.py | rfgsm_test.py | cw_test.py | test_utils.py  Sources    PyPi page for the code\n   Git repo for the code\n  ","date":1588538207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588538207,"objectID":"e8d9236ea1705c1395c98ad7804f4a83","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/deep-illusion/","publishdate":"2020-05-03T13:36:47-07:00","relpermalink":"/~metehancekic/project/deep-illusion/","section":"project","summary":"Created and published an adversarial ML toolbox 'deepillusion'","tags":["Deep Learning"],"title":"Deep Illusion (Adversarial ML toolbox)","type":"project"},{"authors":["Metehan Cekic","Soorya Gopalakrishnan"],"categories":["Deep Learning"],"content":"","date":1583449288,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583449288,"objectID":"edede198007f6e690bb057cd6852ca27","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/adversarial-machine-learning/","publishdate":"2020-03-05T15:01:28-08:00","relpermalink":"/~metehancekic/project/adversarial-machine-learning/","section":"project","summary":"Combating adversarial perturbations by exploiting sparsity of natural data.","tags":["Deep Learning"],"title":"Sparsifying Front Ends","type":"project"},{"authors":["Metehan Cekic","Can Bakiskan"],"categories":[],"content":"The Batak Game Batak is a card game widely played in Turkey. It is similar to Bridge, where after the deck is dealt, players bid -based on their judgment of the value of their hand- in an auction to determine the trump suit. In Batak, each play consists of 13 tricks. A trick is where each player plays 1 card. The play goes as follows: in the first trick, the player that won the auction starts. They play a card from a non-trump suit. Each player has to play a card from the same suit and increase the card’s value. If they can’t increase they can play a lower value card of the same suit. If they don’t have any card of that suit, they can play a card from the trump suit. If they don’t have any card from the trump suit, they can play a card from any other suit they have. In each trick afterwards, the winner of the previous trick starts and other players play according to the same rules outlined above. There are two important rules. Number one, if you have a card of the same suit as the initial suit of the trick, you have to play and you have to increase if you can. Number two, players can’t play the trump suit as the first card of a trick unless trump suit has been \u0026ldquo;unlocked\u0026rdquo; by a player in an earlier trick by him/her not having the suit of that trick, and playing trump suit. In the end, scores are assigned by the number of tricks each player won. If the winner of the auction scored less than their bid, they are assigned negative of the bid as the score. So the advantage of winning the auction is being able to determine the trump suit, the disadvantage is the high negative score in the case of failure to win tricks as many as the bid. In a game, there are usually 10 plays, the goal of the game is to get the highest score.\nAdvantage Actor-Critic Algorithm (A2C) The main property of the A2C algorithm is that it not only optimizes a policy but also trains a value function for the current state to assess how good is that policy for that specific state. In general, policy-based methods and value-based methods have a number of upsides and downsides. A2C algorithm is designed to combine the benefits of both value based and policy based approaches.\nValue-based algorithms require a value function to be calculated for every possible action, although it is more sample efficient. On the other hand, policy-based methods can easily utilized on continuous action spaces due to its direct optimization of policy. However, low sample efficiency causes them to converge slower.\nA2C algorithm estimates both policy and value function via deep neural network. Value function corresponds to the worth of that state to be in and the policy gives the probabilities of each available actions for that state.\n $ \\pi(a|s;\\theta)$: the probability of action $a$ given the state $s$  $V_{\\pi}(s)$ : How good is it to be in state $s$  The value function is mathematically defined as expected reward of a trajectory after the state that agent is at that time in, which is only dependent on $s$.  $V_{\\pi_{\\theta}}(s)=E_{\\tau}(R_{\\tau}|s_0 =s,\\pi_{\\theta})$  Where $\\tau$ represents the trajectory until game ends. Hereafter, for simplicity we drop $\\theta$ from $\\pi$, which means $\\pi$ is parameterized by $\\theta$. Critic of the agent evaluates the policies by computing the advantage function of specific action in a given state, which is defined as follow: $A_{\\pi}(s,a) = Q_{\\pi}(s,a)−V_{\\pi}(s)$ : The advantage of making action a in state s  Where Q function is another value function which depends on both state and action, particularly it corresponds to the value of an action $a$ at a state $s$. Observe that with given definitions: $V_{\\pi}(s) = 􏰀 \\sum_{a\\in A}\\pi(a|s)Q_{\\pi} (s, a)$  The algorithm utilizes the fact that there is a trajectory obtained from a game play (An agent interacted with the environment with a given policy function) by computing Q values from empirical discounted reward:\n$Q_{\\pi}(s,a) = R =\\sum_{i=0}r_{t_i} \\gamma^i $  where $\\gamma$ corresponds to the how important are the future rewards compared to the current reward (1 means equally important, whereas 0.5 means each time step decreases the value of future reward by half). As we have 2 different outputs for our deep neural network, namely policy and value functions, we have two different loss functions. The value loss is defined as mean squared error (MSE) between empirical discounted reward and the value output. This loss motivates neural net to learn to assess policy better after each iteration.\n$Loss_{value} =􏰀\\sum(R−V_{\\pi}(s;\\theta_v))^2$  Similarly, we want to increase the probability of an action which has positive advantage to increase empirical reward, therefore policy loss is defined by: $Loss_{policy} =\\log\\frac{1}{\\pi(a|s;\\theta_p)} A_{\\pi}(s,a;\\theta_v)−\\beta H(\\pi)$  The loss function includes the last term to increase entropy of the action distribution, in other words, we push neural network to explore more. Typical $\\beta$ value is 0.001, it is a good hyperparamemeter to adjust between exploration and exploitation which is a dilemma we always encounter in reinforcement learning algorithms. More inclination towards higher $\\beta$ values causes neural net to explore more, whereas low $\\beta$ values stick with the locally optimum policy. Asynchronous Advantage Actor-Critic Algorithm (A3C) Asynchronous gradient descent boosts the performance of traditional reinforcement algorithms like A2C and Q learning. The main idea of framework is having a master neural network and updating it with the gradients computed from some other worker networks. In standard reinforcement algorithms we usually use memory mechanism to feed earlier experience of the agent when updating the neural network. We call this concept as \u0026ldquo;experience replay\u0026rdquo;, which is helpful to stabilize learning. Running parallel agents independently stabilizes learning without \u0026ldquo;experience replay\u0026rdquo; because of the stochasticity in the game each agent experiences totally different action-reward trajectory. Moreover, as all agents play the game in parallel, training time is reduced roughly linear in the number of parallel actors.\nOur Model We decided that in order to capture the sequential information within the game we should either keep a list of all played cards explicitly in some form of memory, or we should use an RNN that would learn to extract the underlying temporal information implicitly. We decided to go with the second option as the first option requires much bigger input sizes which in turn requires bigger number of neurons in the model. With this decision, we determined the essential states of the game that a human player uses when playing and made these the inputs to our model. They are: trump suit (One-hot vector of length 4), hand (Boolean vector of length 52) and current and past trick’s cards up to our model’s turn, along with who played them.(Boolean matrix of size 7 by 56) Cards are represented by one-hot vector of length 52, players are represented by one-hot vector of length 4, then these vectors are concatenated to form vector of size 56.\nIn following figure, overall architecture of our model is displayed, with the LSTM unrolled in time to give a better sense of time dependency.\nResults We trained 3 different architectures with Nvidia GeForce GTX 1080 Ti. To be able to evaluate trained agent, we get a couple of people (Who knows and plays the game very well) play the game against random-playing bots similar to trained agent.\nResults are given in following figure. As can be seen from the figure, all architectures we experimented performs similarly and much better than random playing games. However, all architectures perform slightly worse than humans.\nConclusion In this project, we wanted to implement and evaluate the performance of A3C algorithm on the Turkish game called \u0026ldquo;Batak\u0026rdquo;. We also wanted to observe the effect of architecture and optimization method for convergence. As can be seen from the results section, the architectures we tried so far perform more or less same.\nWe trained agent in the game without auction, next step for this project would be to improve bots to be able play with auction (learn and bid accordingly). Moreover, we plan to develop agent by making it play against itself (Instead of random bots).\n","date":1583017002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583017002,"objectID":"b0145d23755ce12d801a161f11d3cad1","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/reinforcement-learning-batak/","publishdate":"2020-02-29T14:56:42-08:00","relpermalink":"/~metehancekic/project/reinforcement-learning-batak/","section":"project","summary":"Teaching a network to play Turkish card game “Batak” by using Reinforcement Learning methods. Asynchronous Advantage Actor Critic (A3C) algorithm is implemented for learning. Game environment for agents is written from scratch. LSTM based network model is used to capture temporal information.","tags":["Deep Learning"],"title":"Reinforcement Learning For The Game Called \"Batak\"","type":"project"},{"authors":["Metehan Cekic","Soorya Gopalakrishnan"],"categories":["Deep Learning","Wireless Communication"],"content":"Radio Frequency Machine Learning  Our goal is to learn\u0026nbsp;RF signatures\u0026nbsp;that can distinguish between devices sending\u0026nbsp;exactly\u0026nbsp;the same message. This is possible due to subtle hardware imperfections (labeled\u0026nbsp;\"nonlinearities\" in the figure below) unique to each device.   Since the information in RF data resides in complex baseband, we employ CNNs with complex-valued weights to learn these signatures. This technique\u0026nbsp;does\u0026nbsp;not use\u0026nbsp;signal domain knowledge and can be used for any wireless protocol. We demonstrate its effectiveness for two protocols -\u0026nbsp;WiFi and ADS-B.   We show that this\u0026nbsp;approach is vulnerable to spoofing\u0026nbsp;when using\u0026nbsp;the entire packet:\u0026nbsp;the CNN focuses on\u0026nbsp;fields containing ID info (eg. MAC ID in WiFi) which can be easily spoofed. When using the preamble alone, reasonably high accuracies are obtained, and performance is significantly enhanced by noise augmentation.   We also study robustness to confounding factors\u0026nbsp;in data collected over multiple days and locations, such as the carrier frequency offset (CFO), which drifts over time, and the wireless channel, which depends on the propagation environment. We show that carefully designed data augmentation is critical for learning robust wireless signatures.  \u0026nbsp;\nPublications Metehan Cekic*,\u0026nbsp;Soorya Gopalakrishnan*,\u0026nbsp;Upamanyu Madhow, \"Robust Wireless Fingerprinting: Generalizing Across Space and Time\", arXiv:2002.10791.       Soorya Gopalakrishnan*,  Metehan Cekic* ,\u0026nbsp;Upamanyu Madhow, \"Robust Wireless Fingerprinting via Complex-Valued Neural Networks\", IEEE Global Communications Conference (Globecom),\u0026nbsp;Waikoloa, Hawaii, Dec. 2019.     ","date":1582670974,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582670974,"objectID":"e47c116437f582cd66bd2091001f7d45","permalink":"https://www.ece.ucsb.edu/~metehancekic/project/mywireless-fingerprinting/","publishdate":"2020-02-25T14:49:34-08:00","relpermalink":"/~metehancekic/project/mywireless-fingerprinting/","section":"project","summary":"Extracting circuit-level fingerprints to distinguish between wireless devices sending exactly same message","tags":["Deep Learning","Wireless Communication"],"title":"Radio Frequency Machine Learning (RFML)","type":"project"},{"authors":["Can Bakiskan","Soorya Gopalakrishnan","Metehan Cekic","Upamanyu Madhow","Ramtin Pedarsani"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a485c61333ee5fff2ae5be7419e7702e","permalink":"https://www.ece.ucsb.edu/~metehancekic/publication/bakiskan-2020-polarizing/","publishdate":"2020-02-29T04:44:16.123687Z","relpermalink":"/~metehancekic/publication/bakiskan-2020-polarizing/","section":"publication","summary":"","tags":null,"title":"Polarizing Front Ends for Robust CNNs","type":"publication"},{"authors":["Metehan Cekic","Soorya Gopalakrishnan","Upamanyu Madhow"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f7d0eabf98f7b67d5c322131c35a8e6b","permalink":"https://www.ece.ucsb.edu/~metehancekic/publication/cekic-2020-robust/","publishdate":"2020-02-29T04:44:16.123149Z","relpermalink":"/~metehancekic/publication/cekic-2020-robust/","section":"publication","summary":"","tags":null,"title":"Robust Wireless Fingerprinting: Generalizing Across Space and Time","type":"publication"},{"authors":["Soorya Gopalakrishnan","Metehan Cekic","Upamanyu Madhow"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"fe1ee3c77a8bfe9394ff4940ac777fc5","permalink":"https://www.ece.ucsb.edu/~metehancekic/publication/fingerprinting-2019-globecom/","publishdate":"2020-02-29T04:44:16.122164Z","relpermalink":"/~metehancekic/publication/fingerprinting-2019-globecom/","section":"publication","summary":"","tags":null,"title":"Robust Wireless Fingerprinting via Complex-Valued Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"https://www.ece.ucsb.edu/~metehancekic/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/~metehancekic/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]