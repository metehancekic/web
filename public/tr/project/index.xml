<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Metehan Cekic</title>
    <link>https://www.ece.ucsb.edu/~metehancekic/tr/project/</link>
      <atom:link href="https://www.ece.ucsb.edu/~metehancekic/tr/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>tr</language><lastBuildDate>Sun, 03 May 2020 13:36:47 -0700</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=true) shape:circle]</url>
      <title>Projects</title>
      <link>https://www.ece.ucsb.edu/~metehancekic/tr/project/</link>
    </image>
    
    <item>
      <title>Derin İllüzyon (Saldırgan Makine Öğrenmesi)</title>
      <link>https://www.ece.ucsb.edu/~metehancekic/tr/project/deep-illusion/</link>
      <pubDate>Sun, 03 May 2020 13:36:47 -0700</pubDate>
      <guid>https://www.ece.ucsb.edu/~metehancekic/tr/project/deep-illusion/</guid>
      <description>&lt;br/&gt;
&lt;h2 id=&#34;deep-illusion-&#34;&gt;Deep Illusion&lt;/h2&gt;
&lt;p&gt;Deep Illusion is a toolbox for adversarial attacks in machine learning. Current version is only implemented for Pytorch models. DeepIllusion is a growing and developing python module which aims to help adversarial machine learning community to accelerate their research. Module currently includes complete implementation of well-known attacks (PGD, FGSM, R-FGSM, CW, BIM etc..). All attacks have an apex(amp) version which you can run your attacks fast and accurately. We strongly recommend that amp versions should only be used for adversarial training since it may have gradient masking issues after neural net gets confident about its decisions. All attack methods have an option (Verbose: False) to check if gradient masking is happening.&lt;/p&gt;
&lt;p&gt;All attack codes are written in functional programming style, therefore, users can easily call the method function and feed the input data and model to get perturbations. All codes are documented, and contains the example use in their description. Users can easily access the documentation by typing &amp;ldquo;??&amp;rdquo; at the and of the method they want to use in Ipython (E.g FGSM?? or PGD??). Output perturbations are already clipped for each image to prevent illegal pixel values. We are open to contributers to expand the attack methods arsenal.&lt;/p&gt;
&lt;p&gt;We also include the most effective current approach to defend DNNs against adversarial perturbations which is training the network using adversarially perturbed examples. Adversarial training and testing methods are included in torchdefenses submodule.&lt;/p&gt;
&lt;p&gt;To standardize the arguments for all attacks, methods accept attack parameters as a dictionary named as attack_params which contains the necessary parameters for each attack. Furthermore, attack methods get the data properties such as the maximum and the minimum pixel value as another dictionary named data_params. These dictinaries make function calls concise and standard for all methods.&lt;/p&gt;
&lt;p&gt;Current version is tested with different defense methods and the standard models for verification and we observed the reported accuracies.&lt;/p&gt;
&lt;p&gt;Code can be installed via PyPi:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install deepillusion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;a href=&#34;https://pypi.org/project/deepillusion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyPi page for the code&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/metehancekic/deep-illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git repo for the code&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Saldırgan Makine Öğrenmesi</title>
      <link>https://www.ece.ucsb.edu/~metehancekic/tr/project/adversarial-machine-learning/</link>
      <pubDate>Thu, 05 Mar 2020 15:01:28 -0800</pubDate>
      <guid>https://www.ece.ucsb.edu/~metehancekic/tr/project/adversarial-machine-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>&#34;Batak&#34; oyunu için Takviyeli Öğrenme</title>
      <link>https://www.ece.ucsb.edu/~metehancekic/tr/project/reinforcement-learning-batak/</link>
      <pubDate>Sat, 29 Feb 2020 14:56:42 -0800</pubDate>
      <guid>https://www.ece.ucsb.edu/~metehancekic/tr/project/reinforcement-learning-batak/</guid>
      <description>&lt;h2 id=&#34;the-batak-game-&#34;&gt;&lt;strong&gt;The Batak Game&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Batak is a card game widely played in Turkey. It is similar to Bridge, where after the deck is dealt, players bid -based on their judgment of the value of their hand- in an auction to determine the &lt;em&gt;trump&lt;/em&gt; suit. In Batak, each &lt;em&gt;play&lt;/em&gt; consists of 13 &lt;em&gt;tricks&lt;/em&gt;. A trick is where each player plays 1 card. The play goes as follows: in the first trick, the player that won the auction starts. They play a card from a non-trump suit. Each player has to play a card from the same suit and increase the card’s value. If they can’t increase they can play a lower value card of the same suit. If they don’t have any card of that suit, they can play a card from the trump suit. If they don’t have any card from the trump suit, they can play a card from any other suit they have. In each trick afterwards, the winner of the previous trick starts and other players play according to the same rules outlined above. There are two important rules. Number one, if you have a card of the same suit as the initial suit of the trick, you have to play and you have to increase if you can. Number two, players can’t play the trump suit as the first card of a trick unless trump suit has been &amp;ldquo;unlocked&amp;rdquo; by a player in an earlier trick by him/her not having the suit of that trick, and playing trump suit. In the end, scores are assigned by the number of tricks each player won. If the winner of the auction scored less than their bid, they are assigned negative of the bid as the score. So the advantage of winning the auction is being able to determine the trump suit, the disadvantage is the high negative score in the case of failure to win tricks as many as the bid. In a game, there are usually 10 plays, the goal of the game is to get the highest score.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;Simply Easy Learning&#34; width=&#34;600&#34;
height=&#34;240&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;advantage-actor-critic-algorithm-a2c-&#34;&gt;&lt;strong&gt;Advantage Actor-Critic Algorithm (A2C)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The main property of the A2C algorithm is that it not only optimizes a policy but also trains a value function for the current state to assess how good is that policy for that specific state. In general, policy-based methods and value-based methods have a number of upsides and downsides. A2C algorithm is designed to combine the benefits of both value based and policy based approaches.&lt;/p&gt;
&lt;p&gt;Value-based algorithms require a value function to be calculated for every possible action, although it is more sample efficient. On the other hand, policy-based methods can easily utilized on continuous action spaces due to its direct optimization of policy. However, low sample efficiency causes them to converge slower.&lt;/p&gt;
&lt;p&gt;A2C algorithm estimates both policy and value function via deep neural network. Value function corresponds to the worth of that state to be in and the policy gives the probabilities of each available actions for that state.&lt;/p&gt;
&lt;center&gt; $ \pi(a|s;\theta)$: the probability of action $a$ given the state $s$ &lt;/center&gt;
	&lt;center&gt;$V_{\pi}(s)$ : How good is it to be in state $s$&lt;/center&gt;
&lt;br/&gt;
The value function is mathematically defined as expected reward of a trajectory after the state that agent is at that time in, which is only dependent on $s$.
&lt;br/&gt;
&lt;center&gt;$V_{\pi_{\theta}}(s)=E_{\tau}(R_{\tau}|s_0 =s,\pi_{\theta})$&lt;/center&gt;
&lt;br/&gt;
Where $\tau$ represents the trajectory until game ends. Hereafter, for simplicity we drop $\theta$ from $\pi$, which means $\pi$ is parameterized by $\theta$. Critic of the agent evaluates the policies by computing the advantage function of specific action in a given state, which is defined as follow:
&lt;center&gt;$A_{\pi}(s,a) = Q_{\pi}(s,a)−V_{\pi}(s)$ : The advantage of making action a in state s&lt;/center&gt;
&lt;br/&gt;
Where Q function is another value function which depends on both state and action, particularly it
corresponds to the value of an action $a$ at a state $s$. Observe that with given definitions: 
&lt;center&gt;$V_{\pi}(s) = 􏰀 \sum_{a\in A}\pi(a|s)Q_{\pi} (s, a)$&lt;/center&gt;
&lt;br/&gt;
&lt;p&gt;The algorithm utilizes the fact that there is a trajectory obtained from a game play (An agent interacted with the environment with a given policy function) by computing Q values from empirical discounted reward:&lt;/p&gt;
&lt;center&gt;$Q_{\pi}(s,a) = R =\sum_{i=0}r_{t_i} \gamma^i $&lt;/center&gt;
&lt;br/&gt;
where $\gamma$ corresponds to the how important are the future rewards compared to the current reward (1 means equally important, whereas 0.5 means each time step decreases the value of future reward by half).
&lt;p&gt;As we have 2 different outputs for our deep neural network, namely policy and value functions, we have two different loss functions. The value loss is defined as mean squared error (MSE) between empirical discounted reward and the value output. This loss motivates neural net to learn to assess policy better after each iteration.&lt;/p&gt;
&lt;center&gt;$Loss_{value} =􏰀\sum(R−V_{\pi}(s;\theta_v))^2$&lt;/center&gt;
&lt;br/&gt;
Similarly, we want to increase the probability of an action which has positive advantage to increase empirical reward, therefore policy loss is defined by:
&lt;center&gt;$Loss_{policy} =\log\frac{1}{\pi(a|s;\theta_p)} A_{\pi}(s,a;\theta_v)−\beta H(\pi)$&lt;/center&gt;
&lt;br/&gt;
The loss function includes the last term to increase entropy of the action distribution, in other words, we push neural network to explore more. Typical $\beta$ value is 0.001, it is a good hyperparamemeter to adjust between exploration and exploitation which is a dilemma we always encounter in reinforcement learning algorithms. More inclination towards higher $\beta$ values causes neural net to explore more, whereas low $\beta$ values stick with the locally optimum policy.
&lt;h2 id=&#34;asynchronous-advantage-actor-critic-algorithm-a3c-&#34;&gt;&lt;strong&gt;Asynchronous Advantage Actor-Critic Algorithm (A3C)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Asynchronous gradient descent boosts the performance of traditional reinforcement algorithms like A2C and Q learning. The main idea of framework is having a master neural network and updating it with the gradients computed from some other worker networks.
In standard reinforcement algorithms we usually use memory mechanism to feed earlier experience of the agent when updating the neural network. We call this concept as &amp;ldquo;experience replay&amp;rdquo;, which is helpful to stabilize learning. Running parallel agents independently stabilizes learning without &amp;ldquo;experience replay&amp;rdquo; because of the stochasticity in the game each agent experiences totally different action-reward trajectory. Moreover, as all agents play the game in parallel, training time is reduced roughly linear in the number of parallel actors.&lt;/p&gt;
&lt;h2 id=&#34;our-model-&#34;&gt;&lt;strong&gt;Our Model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We decided that in order to capture the sequential information within the game we should either keep a list of all played cards explicitly in some form of memory, or we should use an RNN that would learn to extract the underlying temporal information implicitly. We decided to go with the second option as the first option requires much bigger input sizes which in turn requires bigger number of neurons in the model. With this decision, we determined the essential states of the game that a human player uses when playing and made these the inputs to our model. They are: trump suit (One-hot vector of length 4), hand (Boolean vector of length 52) and current and past trick’s cards up to our model’s turn, along with who played them.(Boolean matrix of size 7 by 56) Cards are represented by one-hot vector of length 52, players are represented by one-hot vector of length 4, then these vectors are concatenated to form vector of size 56.&lt;/p&gt;
&lt;p&gt;In following figure, overall architecture of our model is displayed, with the LSTM unrolled in time to give a better sense of time dependency.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;arch.png&#34; alt=&#34;Simply Easy Learning&#34; width=&#34;600&#34;
height=&#34;240&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;results-&#34;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We trained 3 different architectures with Nvidia GeForce GTX 1080 Ti. To be able to evaluate trained agent, we get a couple of people (Who knows and plays the game very well) play the game against random-playing bots similar to trained agent.&lt;/p&gt;
&lt;p&gt;Results are given in following figure. As can be seen from the figure, all architectures we experimented performs similarly and much better than random playing games. However, all architectures perform slightly worse than humans.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;results.png&#34; alt=&#34;Simply Easy Learning&#34; width=&#34;600&#34;
height=&#34;240&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion-&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this project, we wanted to implement and evaluate the performance of A3C algorithm on the Turkish game called &amp;ldquo;Batak&amp;rdquo;. We also wanted to observe the effect of architecture and optimization method for convergence. As can be seen from the results section, the architectures we tried so far perform more or less same.&lt;/p&gt;
&lt;p&gt;We trained agent in the game without auction, next step for this project would be to improve bots to be able play with auction (learn and bid accordingly). Moreover, we plan to develop agent by making it play against itself (Instead of random bots).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Radyo Frekansı Makine Öğrenmesi</title>
      <link>https://www.ece.ucsb.edu/~metehancekic/tr/project/mywireless-fingerprinting/</link>
      <pubDate>Tue, 25 Feb 2020 14:49:34 -0800</pubDate>
      <guid>https://www.ece.ucsb.edu/~metehancekic/tr/project/mywireless-fingerprinting/</guid>
      <description>&lt;h2 id=&#34;radio-frequency-machine-learning-&#34;&gt;&lt;strong&gt;Radio Frequency Machine Learning&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
	&lt;li&gt;Our goal is to learn&amp;nbsp;&lt;strong&gt;&lt;em&gt;RF signatures&lt;/em&gt;&lt;/strong&gt;&amp;nbsp;that can distinguish between devices sending&amp;nbsp;&lt;em&gt;exactly&lt;/em&gt;&amp;nbsp;the same message. This is possible due to subtle hardware imperfections (labeled&amp;nbsp;&#34;nonlinearities&#34; in the figure below) unique to each device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;Simply Easy Learning&#34; width=&#34;600&#34;
height=&#34;240&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Since the information in RF data resides in complex baseband, we employ CNNs with complex-valued weights to learn these signatures. This technique&amp;nbsp;does&amp;nbsp;not use&amp;nbsp;signal domain knowledge and can be used for any wireless protocol. We demonstrate its effectiveness for two protocols -&amp;nbsp;WiFi and ADS-B.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;architect.png&#34; alt=&#34;Simply Easy Learning&#34; width=&#34;600&#34;
height=&#34;240&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;We show that this&amp;nbsp;approach is vulnerable to spoofing&amp;nbsp;when using&amp;nbsp;the entire packet:&amp;nbsp;the CNN focuses on&amp;nbsp;fields containing ID info (eg. MAC ID in WiFi) which can be easily spoofed. When using the preamble alone, reasonably high accuracies are obtained, and performance is significantly enhanced by noise augmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;We also study robustness to confounding factors&amp;nbsp;in data collected over multiple days and locations, such as the carrier frequency offset (CFO), which drifts over time, and the wireless channel, which depends on the propagation environment. We show that carefully designed data augmentation is critical for learning robust wireless signatures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;small&gt;&amp;nbsp;&lt;/small&gt;&lt;/p&gt;
&lt;div class=&#34;field__item odd&#34;&gt;&lt;div class=&#34;entity entity-paragraphs-item paragraphs-item-related-publications-topic-group&#34;&gt;
  &lt;div class=&#34;content&#34;&gt;
    &lt;div class=&#34;field field--name-field-sub-topic-title field--type-text field--label-hidden&#34;&gt;&lt;div class=&#34;field__items&#34;&gt;&lt;div class=&#34;field__item even&#34;&gt;&lt;h3 class=&#34;&#34;&gt;Publications&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&#34;field field--name-field-related-publications field--type-entityreference field--label-hidden&#34;&gt;&lt;div class=&#34;field__items&#34;&gt;&lt;div class=&#34;field__item even&#34;&gt;	&lt;div role=&#34;article&#34; class=&#34;node node--simple-publication node--promoted contextual-links-region node--citation node--simple-publication--citation&#34;&gt;
	  &lt;div class=&#34;node__content&#34;&gt;
	    &lt;div class=&#34;field citation&#34;&gt; &lt;b&gt;Metehan Cekic*&lt;/b&gt;,&amp;nbsp;Soorya Gopalakrishnan*,&amp;nbsp;Upamanyu Madhow, &#34;&lt;a href=&#34;https://web.ece.ucsb.edu/~metehancekic/publication/cekic-2020-robust/&#34; target=&#34;_self&#34;&gt;Robust Wireless Fingerprinting: Generalizing Across Space and Time&lt;/a&gt;&#34;, arXiv:2002.10791.
&lt;br/&gt;
&lt;br/&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;field field--name-field-related-publications field--type-entityreference field--label-hidden&#34;&gt;&lt;div class=&#34;field__items&#34;&gt;&lt;div class=&#34;field__item even&#34;&gt;	&lt;div role=&#34;article&#34; class=&#34;node node--simple-publication node--promoted contextual-links-region node--citation node--simple-publication--citation&#34;&gt;
	  &lt;div class=&#34;node__content&#34;&gt;
	    &lt;div class=&#34;field citation&#34;&gt;Soorya Gopalakrishnan*, &lt;b&gt; Metehan Cekic* &lt;/b&gt;,&amp;nbsp;Upamanyu Madhow, &#34;&lt;a href=&#34;https://www.ece.ucsb.edu/~metehancekic/publication/fingerprinting-2019-globecom/&#34; target=&#34;_self&#34;&gt;Robust Wireless Fingerprinting via Complex-Valued Neural Networks&lt;/a&gt;&#34;, &lt;i&gt;IEEE Global Communications Conference (Globecom)&lt;/i&gt;,&amp;nbsp;Waikoloa, Hawaii, Dec. 2019. 
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
